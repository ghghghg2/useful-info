
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8" />
  <title>Memory Barriers Are Like Source Control Operations</title>
  <meta name="author" content="Jeff Preshing" />

  
  
  <meta name="description" content="If you use source control, you&rsquo;re on your way towards understanding memory ordering, an important consideration when writing lock-free code in C, C++ and other languages. In &hellip;" />
  
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True" />
  <meta name="MobileOptimized" content="320" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <link rel="canonical" href="https://preshing.com/20120710/memory-barriers-are-like-source-control-operations" />
  <link href="/favicon.png" rel="icon" />
  <link href="/stylesheets/screen.css?v2" media="screen, projection" rel="stylesheet" type="text/css" />
  <link href="/feed" rel="alternate" title="Preshing on Programming" type="application/atom+xml" />
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/lib/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-44017752-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/"><img src="/images/header.png" alt="Preshing on Programming"/>Preshing on Programming</a></h1>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="https://twitter.com/preshing" rel="follow-twitter" title="Follow on Twitter">Twitter</a></li>
  <li><a href="/feed" rel="subscribe-rss" title="Subscribe via RSS">RSS</a></li>
  
</ul>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/archives">Archives</a></li>
  <li><a href="/about">About</a></li>
  <li><a href="/contact">Contact</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      <p class="meta">
        








  


<time datetime="2012-07-10T06:40:01-04:00" pubdate data-updated="true">Jul 10, 2012</time>
        
      </p>
    
    
      <h1 class="entry-title">Memory Barriers Are Like Source Control Operations</h1>
    
  </header>


<div class="entry-content"><p>If you use source control, you&rsquo;re on your way towards understanding memory ordering, an important consideration when writing lock-free code in C, C++ and other languages.</p>

<p>In my last post, I wrote about <a href="http://preshing.com/20120625/memory-ordering-at-compile-time">memory ordering at compile time</a>, which forms one half of the memory ordering puzzle. This post is about the other half: memory ordering at runtime, on the processor itself. Like compiler reordering, processor reordering is invisible to a single-threaded program. It only becomes apparent when <a href="http://preshing.com/20120612/an-introduction-to-lock-free-programming">lock-free techniques</a> are used &ndash; that is, when shared memory is manipulated without any mutual exclusion between threads. However, unlike compiler reordering, the effects of processor reordering are <a href="http://preshing.com/20120515/memory-reordering-caught-in-the-act">only visible in multicore and multiprocessor systems</a>.</p>

<p>You can enforce correct memory ordering on the processor by issuing any instruction which acts as a <strong>memory barrier</strong>. In some ways, this is the only technique you need to know, because when you use such instructions, compiler ordering is taken care of automatically. Examples of instructions which act as memory barriers include (but are not limited to) the following:</p>

<ul>
  <li>Certain inline assembly directives in GCC, such as the PowerPC-specific <code>asm volatile("lwsync" ::: "memory")</code></li>
  <li>Any <a href="http://msdn.microsoft.com/en-us/library/windows/desktop/ms684122.aspx">Win32 Interlocked operation</a>, except on Xbox 360</li>
  <li>Many operations on <a href="http://en.cppreference.com/w/cpp/atomic/atomic">C++11 atomic types</a>, such as <code>load(std::memory_order_acquire)</code></li>
  <li>Operations on POSIX mutexes, such as <a href="http://linux.die.net/man/3/pthread_mutex_lock"><code>pthread_mutex_lock</code></a></li>
</ul>

<!-- more -->
<p>Just as there are many instructions which act as memory barriers, there are many different types of memory barriers to know about. Indeed, not all of the above instructions produce the same kind of memory barrier &ndash; leading to another possible area of confusion when writing lock-free code. In an attempt to clear things up to some extent, I&rsquo;d like to offer an analogy which I&rsquo;ve found helpful in understanding the vast majority (but not all) of possible memory barrier types.</p>

<p>To begin with, consider the architecture of a typical multicore system. Here&rsquo;s a device with two cores, each having 32 KiB of private L1 data cache. There&rsquo;s 1 MiB of L2 cache shared between both cores, and 512 MiB of main memory.</p>

<p><img class="center" src="/images/cpu-diagram.png" /></p>

<p>A multicore system is a bit like a group of programmers collaborating on a project using a bizarre kind of source control strategy. For example, the above dual-core system corresponds to a scenario with just two programmers. Let&rsquo;s name them Larry and Sergey.</p>

<p><img class="center" src="/images/source-control-analogy.png" /></p>

<p>On the right, we have a shared, central repository &ndash; this represents a combination of main memory and the shared L2 cache. Larry has a complete working copy of the repository on his local machine, and so does Sergey &ndash; these (effectively) represent the L1 caches attached to each CPU core. There&rsquo;s also a scratch area on each machine, to privately keep track of registers and/or local variables. Our two programmers sit there, feverishly editing their working copy and scratch area, all while making decisions about what to do next based on the data they see &ndash; much like a thread of execution running on that core.</p>

<p>Which brings us to the source control strategy. In this analogy, the source control strategy is very strange indeed. As Larry and Sergey modify their working copies of the repository, their modifications are constantly <strong>leaking</strong> in the background, to and from the central repository, at totally random times. Once Larry edits the file X, his change will leak to the central repository, but there&rsquo;s no guarantee about when it will happen. It might happen immediately, or it might happen much, much later. He might go on to edit other files, say Y and Z, and those modifications might leak into the respository <em>before</em> X gets leaked. In this manner, stores are effectively reordered on their way to the repository.</p>

<p>Similarly, on Sergey&rsquo;s machine, there&rsquo;s no guarantee about the timing or the order in which those changes leak <em>back</em> from the repository into <em>his</em> working copy. In this manner, loads are effectively reordered on their way out of the repository.</p>

<p>Now, if each programmer works on completely separate parts of the repository, neither programmer will be aware of these background leaks going on, or even of the other programmer&rsquo;s existence. That would be analogous to running two independent, single-threaded processes. In this case, the <a href="http://preshing.com/20120625/memory-ordering-at-compile-time">cardinal rule of memory ordering</a> is upheld.</p>

<p>The analogy becomes more useful once our programmers start working on the same parts of the repository. Let&rsquo;s revisit the example I gave <a href="http://preshing.com/20120515/memory-reordering-caught-in-the-act">in an earlier post</a>. X and Y are global variables, both initially 0:</p>

<p><img class="center" src="/images/marked-example2-2.png" /></p>

<p>Think of X and Y as files which exist on Larry&rsquo;s working copy of the repository, Sergey&rsquo;s working copy, and the central repository itself. Larry writes 1 to his working copy of X and Sergey writes 1 to his working copy of Y at roughly the same time. If neither modification has time to leak to the repository and back before each programmer looks up his working copy of the <em>other</em> file, they&rsquo;ll end up with both r1 = 0 and r2 = 0. This result, which may have seemed counterintuitive at first, actually becomes pretty obvious in the source control analogy.</p>

<p><img class="center" src="/images/iriw-state.png" /></p>

<h2 id="types-of-memory-barrier">Types of Memory Barrier</h2>

<p>Fortunately, Larry and Sergey are not entirely at the mercy of these random, unpredictable leaks happening in the background. They also have the ability to issue special instructions, called fence instructions, which act as memory barriers. For this analogy, it&rsquo;s sufficient to define four types of memory barrier, and thus four different fence instructions. Each type of memory barrier is named after the type of memory reordering it&rsquo;s designed to prevent: for example, <code>#StoreLoad</code> is designed to prevent the reordering of a store followed by a load.</p>

<p><img class="center" src="/images/barrier-types.png" /></p>

<p>As <a href="http://g.oswego.edu/dl/jmm/cookbook.html">Doug Lea points out</a>, these four categories map pretty well to specific instructions on real CPUs &ndash; though not exactly. Most of the time, a real CPU instruction acts as some combination of the above barrier types, possibly in addition to other effects. In any case, once you understand these four types of memory barriers in the source control analogy, you&rsquo;re in a good position to understand a large number of instructions on real CPUs, as well as several higher-level programming language constructs.</p>

<h3 id="loadload">#LoadLoad</h3>

<p>A LoadLoad barrier effectively prevents reordering of loads performed before the barrier with loads performed after the barrier.</p>

<p>In our analogy, the <code>#LoadLoad</code> fence instruction is basically equivalent to a <strong>pull</strong> from the central repository. Think <code>git pull</code>, <code>hg pull</code>, <code>p4 sync</code>, <code>svn update</code> or <code>cvs update</code>, all acting on the entire repository. If there are any merge conflicts with his local changes, let&rsquo;s just say they&rsquo;re resolved randomly.</p>

<p><img class="center" src="/images/loadload.png" /></p>

<p>Mind you, there&rsquo;s no guarantee that <code>#LoadLoad</code> will pull the latest, or head, revision of the entire repository! It could very well pull an older revision than the head, as long as that revision is <em>at least as new as the newest value which leaked from the central repository into his local machine</em>.</p>

<p>This may sound like a weak guarantee, but it&rsquo;s still a perfectly good way to prevent seeing stale data. Consider the classic example, where Sergey checks a shared flag to see if some data has been published by Larry. If the flag is true, he issues a <code>#LoadLoad</code> barrier before reading the published value:</p>

<div><div class="CodeRay">
  <div class="code"><pre><span class="keyword">if</span> (IsPublished)                   <span class="comment">// Load and check shared flag</span>
{
    LOADLOAD_FENCE();              <span class="comment">// Prevent reordering of loads</span>
    <span class="keyword">return</span> Value;                  <span class="comment">// Load published value</span>
}
</pre></div>
</div>
</div>

<p>Obviously, this example depends on having the <code>IsPublished</code> flag leak into Sergey&rsquo;s working copy by itself. It doesn&rsquo;t matter exactly when that happens; once the leaked flag has been observed, he issues a <code>#LoadLoad</code> fence to prevent reading some value of <code>Value</code> which is older than the flag itself.</p>

<h3 id="storestore">#StoreStore</h3>

<p>A StoreStore barrier effectively prevents reordering of stores performed before the barrier with stores performed after the barrier.</p>

<p>In our analogy, the <code>#StoreStore</code> fence instruction corresponds to a <strong>push</strong> to the central repository. Think <code>git push</code>, <code>hg push</code>, <code>p4 submit</code>, <code>svn commit</code> or <code>cvs commit</code>, all acting on the entire repository.</p>

<p><img class="center" src="/images/storestore.png" /></p>

<p>As an added twist, let&rsquo;s suppose that <code>#StoreStore</code> instructions are <strong>not instant</strong>. They&rsquo;re performed in a delayed, asynchronous manner. So, even though Larry executes a <code>#StoreStore</code>, we can&rsquo;t make any assumptions about when all his previous stores finally become visible in the central repository.</p>

<p>This, too, may sound like a weak guarantee, but again, it&rsquo;s perfectly sufficient to prevent Sergey from seeing any stale data published by Larry. Returning to the same example as above, Larry needs only to publish some data to shared memory, issue a <code>#StoreStore</code> barrier, then set the shared flag to true:</p>

<div><div class="CodeRay">
  <div class="code"><pre>Value = x;                         <span class="comment">// Publish some data</span>
STORESTORE_FENCE();
IsPublished = <span class="integer">1</span>;                   <span class="comment">// Set shared flag to indicate availability of data</span>
</pre></div>
</div>
</div>

<p>Again, we&rsquo;re counting on the value of <code>IsPublished</code> to leak from Larry&rsquo;s working copy over to Sergey&rsquo;s, all by itself. Once Sergey detects that, he can be confident he&rsquo;ll see the correct value of <code>Value</code>. What&rsquo;s interesting is that, for this pattern to work, <code>Value</code> does not even need to be an atomic type; it could just as well be a huge structure with lots of elements.</p>

<h3 id="loadstore">#LoadStore</h3>

<p><img class="right" src="/images/get-back-to-later.png" />Unlike <code>#LoadLoad</code> and <code>#StoreStore</code>, there&rsquo;s no clever metaphor for <code>#LoadStore</code> in terms of source control operations. The best way to understand a <code>#LoadStore</code> barrier is, quite simply, in terms of instruction reordering.</p>

<p>Imagine Larry has a set of instructions to follow. Some instructions make him load data from his private working copy into a register, and some make him store data from a register back into the working copy. Larry has the ability to juggle instructions, but only in specific cases. Whenever he encounters a load, he looks ahead at any stores that are coming up after that; if the stores are <em>completely unrelated</em> to the current load, then he&rsquo;s allowed to skip ahead, do the stores first, then come back afterwards to finish up the load. In such cases, the cardinal rule of memory ordering &ndash; never modify the behavior of a single-threaded program &ndash; is still followed.</p>

<p>On a real CPU, such instruction reordering might happen on certain processors if, say, there is a cache miss on the load followed by a cache hit on the store. But in terms of understanding the analogy, such hardware details don&rsquo;t really matter. Let&rsquo;s just say Larry has a boring job, and this is one of the few times when he&rsquo;s allowed to get creative. Whether or not he chooses to do it is completely unpredictable. Fortunately, this is a relatively inexpensive type of reordering to prevent; when Larry encounters a <code>#LoadStore</code> barrier, he simply refrains from such reordering around that barrier.</p>

<p>In our analogy, it&rsquo;s valid for Larry to perform this kind of LoadStore reordering even when there is a <code>#LoadLoad</code> or <code>#StoreStore</code> barrier between the load and the store. However, on a real CPU, instructions which act as a <code>#LoadStore</code> barrier typically act as at least one of those other two barrier types.</p>

<h3 id="storeload">#StoreLoad</h3>

<p>A StoreLoad barrier ensures that all stores performed before the barrier are visible to other processors, and that all loads performed after the barrier receive the latest value that is visible at the time of the barrier. In other words, it effectively prevents reordering of all stores before the barrier against all loads after the barrier, respecting the way a <a href="http://preshing.com/20120612/an-introduction-to-lock-free-programming#sequential-consistency">sequentially consistent</a> multiprocessor would perform those operations.</p>

<p><code>#StoreLoad</code> is unique. It&rsquo;s the only type of memory barrier that will prevent the result r1 = r2 = 0 in the example given in <a href="http://preshing.com/20120515/memory-reordering-caught-in-the-act">Memory Reordering Caught in the Act</a>; the same example I&rsquo;ve repeated earlier in this post.</p>

<p>If you&rsquo;ve been following closely, you might wonder: How is <code>#StoreLoad</code> different from a <code>#StoreStore</code> followed by a <code>#LoadLoad</code>? After all, a <code>#StoreStore</code> pushes changes to the central repository, while <code>#LoadLoad</code> pulls remote changes back. However, those two barrier types are insufficient. Remember, the push operation may be delayed for an arbitrary number of instructions, and the pull operation might not pull from the head revision. This hints at why the PowerPC&rsquo;s <code>lwsync</code> instruction &ndash; which acts as all three <code>#LoadLoad</code>, <code>#LoadStore</code> and <code>#StoreStore</code> memory barriers, but not <code>#StoreLoad</code> &ndash; is insufficient to prevent r1 = r2 = 0 in that example.</p>

<p>In terms of the analogy, a <code>#StoreLoad</code> barrier could be achieved by pushing all local changes to the central repostitory, waiting for that operation to complete, then pulling the absolute latest head revision of the repository. On most processors, instructions that act as a <code>#StoreLoad</code> barrier tend to be more expensive than instructions acting as the other barrier types.</p>

<p><img class="center" src="/images/storeload.png" /></p>

<p>If we throw a <code>#LoadStore</code> barrier into that operation, which shouldn&rsquo;t be a big deal, then what we get is a full memory fence &ndash; acting as all four barrier types at once. <a href="http://g.oswego.edu/dl/jmm/cookbook.html">As Doug Lea also points out</a>, it just so happens that on all current processors, every instruction which acts as a <code>#StoreLoad</code> barrier also acts as a full memory fence.</p>

<h2 id="how-far-does-this-analogy-get-you">How Far Does This Analogy Get You?</h2>

<p>As I&rsquo;ve mentioned previously, <a href="http://preshing.com/20120612/an-introduction-to-lock-free-programming#different-processors-have">every processor has different habits</a> when it comes to memory ordering. The x86/64 family, in particular, has a strong memory model; it&rsquo;s known to keep memory reordering to a minimum. PowerPC and ARM have weaker memory models, and the Alpha is famous for being in a league of its own. Fortunately, the analogy presented in this post corresponds to a <a href="http://preshing.com/20120930/weak-vs-strong-memory-models">weak memory model</a>. If you can wrap your head around it, and enforce correct memory ordering using the fence instructions given here, you should be able to handle most CPUs.</p>

<p>The analogy also corresponds pretty well to the abstract machine targeted by both C++11 (formerly known as C++0x) and C11. Therefore, if you write lock-free code using the standard library of those languages while keeping the above analogy in mind, it&rsquo;s more likely to function correctly on any platform.</p>

<p>In this analogy, I&rsquo;ve said that each programmer represents a single thread of execution running on a separate core. On a real operating system, threads tend to move between different cores over the course of their lifetime, but the analogy still works. I&rsquo;ve also alternated between examples in machine language and examples written in C/C++. Obviously, we&rsquo;d prefer to stick with C/C++, or another high-level language; this is possible because again, any operation which acts as a memory barrier also prevents <a href="http://preshing.com/20120625/memory-ordering-at-compile-time">compiler reordering</a>.</p>

<p>I haven&rsquo;t written about every type of memory barrier yet. For instance, there are also <a href="http://www.mjmwired.net/kernel/Documentation/memory-barriers.txt#305">data dependency barriers</a>. I&rsquo;ll describe those further in a future post. Still, the four types given here are the big ones.</p>

<p>If you&rsquo;re interested in how CPUs work under the hood &ndash; things like stores buffers, cache coherency protocols and other hardware implementation details &ndash; and why they perform memory reordering in the first place, I&rsquo;d recommend the <a href="http://www.rdrop.com/users/paulmck/scalability/paper/whymb.2010.07.23a.pdf">fine</a> <a href="http://www.kernel.org/doc/Documentation/memory-barriers.txt">work</a> of Paul McKenney &amp; David Howells. Indeed, I suspect most programmers who have successfully written lock-free code have at least a passing familiarity with such hardware details.</p>
</div>



  

  <footer>
    <p class="meta">
      
        <a class="basic-alignment left" href="/20120625/memory-ordering-at-compile-time" title="Previous Post: Memory Ordering at Compile Time">&laquo; Memory Ordering at Compile Time</a>
      
      
        <a class="basic-alignment right" href="/20120913/acquire-and-release-semantics" title="Next Post: Acquire and Release Semantics">Acquire and Release Semantics &raquo;</a>
      
    </p>
  </footer>
</article>

<section>
<div id="comment-section">
<script>
var idcomments_acct = 'b741f3bade873745e10e70447d732c8c';
var idcomments_post_id = '3667';
var idcomments_post_url;
</script>
<span id="IDCommentsPostTitle" style="display:none"></span>
<script type='text/javascript' src='https://www.intensedebate.com/js/genericCommentWrapperV2.js'></script>
</div>
</section>


</div>

<aside class="sidebar">
  
    <section>
  <a href="https://plywood.arc80.com/"><p style="margin-bottom:0.8em;">Check out <strong>Plywood</strong>, a cross-platform, open source C++ framework:</p>
  <img srcset="/images/plywood-button.png 1x,/images/plywood-button@2x.png 2x" src="/images/plywood-button.png" width="165"></a>
</section>
<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/20210315/how-cpp-resolves-a-function-call">How C++ Resolves a Function Call</a>
      </li>
    
      <li class="post">
        <a href="/20201210/flap-hero-code-review">Flap Hero Code Review</a>
      </li>
    
      <li class="post">
        <a href="/20201126/a-small-open-source-game-in-cpp">A Small Open Source Game In C++</a>
      </li>
    
      <li class="post">
        <a href="/20200727/automatically-detecting-text-encodings-in-cpp">Automatically Detecting Text Encodings in C++</a>
      </li>
    
      <li class="post">
        <a href="/20200708/io-in-plywood">I/O in Plywood</a>
      </li>
    
      <li class="post">
        <a href="/20200526/a-new-cross-platform-open-source-cpp-framework">A New Cross-Platform Open Source C++ Framework</a>
      </li>
    
      <li class="post">
        <a href="/20180124/a-flexible-reflection-system-in-cpp-part-2">A Flexible Reflection System in C++: Part 2</a>
      </li>
    
      <li class="post">
        <a href="/20180116/a-primitive-reflection-system-in-cpp-part-1">A Flexible Reflection System in C++: Part 1</a>
      </li>
    
      <li class="post">
        <a href="/20171218/how-to-write-your-own-cpp-game-engine">How to Write Your Own C++ Game Engine</a>
      </li>
    
      <li class="post">
        <a href="/20170612/can-reordering-of-release-acquire-operations-introduce-deadlock">Can Reordering of Release/Acquire Operations Introduce Deadlock?</a>
      </li>
    
      <li class="post">
        <a href="/20170529/heres-a-standalone-cairo-dll-for-windows">Here's a Standalone Cairo DLL for Windows</a>
      </li>
    
      <li class="post">
        <a href="/20170522/learn-cmakes-scripting-language-in-15-minutes">Learn CMake's Scripting Language in 15 Minutes</a>
      </li>
    
      <li class="post">
        <a href="/20170511/how-to-build-a-cmake-based-project">How to Build a CMake-Based Project</a>
      </li>
    
      <li class="post">
        <a href="/20160726/using-quiescent-states-to-reclaim-memory">Using Quiescent States to Reclaim Memory</a>
      </li>
    
      <li class="post">
        <a href="/20160314/leapfrog-probing">Leapfrog Probing</a>
      </li>
    
      <li class="post">
        <a href="/20160222/a-resizable-concurrent-map">A Resizable Concurrent Map</a>
      </li>
    
      <li class="post">
        <a href="/20160201/new-concurrent-hash-maps-for-cpp">New Concurrent Hash Maps for C++</a>
      </li>
    
      <li class="post">
        <a href="/20150402/you-can-do-any-kind-of-atomic-read-modify-write-operation">You Can Do Any Kind of Atomic Read-Modify-Write Operation</a>
      </li>
    
      <li class="post">
        <a href="/20150324/safe-bitfields-in-cpp">Safe Bitfields in C++</a>
      </li>
    
      <li class="post">
        <a href="/20150316/semaphores-are-surprisingly-versatile">Semaphores are Surprisingly Versatile</a>
      </li>
    
  </ul>
</section>

  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2021 Jeff Preshing -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  











</body>
</html>
